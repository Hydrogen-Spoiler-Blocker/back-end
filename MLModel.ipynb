{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spoiler blocker.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hydrogen-Spoiler-Blocker/back-end/blob/master/MLModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7geMqzy6-QTt"
      },
      "source": [
        "## Spoiler blocker LSTM model\n",
        "\n",
        "The files used to train the model are uploaded on our personal Google Drive. But can be found and downloaded on these links:  \n",
        "Smaller dataset: https://www.kaggle.com/rmisra/imdb-spoiler-dataset?select=IMDB_reviews.json  \n",
        "Mega dataset: https://www.kaggle.com/ebiswas/imdb-review-dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgwNsTSkbHlU"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k10NOyHAKmUQ"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os.path\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfrC_9qSBAm5"
      },
      "source": [
        "Smaller dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzKvxZ5eLH3t"
      },
      "source": [
        "# path = \"/content/drive/MyDrive/IMDB_reviews.json.zip\"\n",
        "# with zipfile.ZipFile(path, 'r') as zip_ref:\n",
        "#     zip_ref.extractall(\"/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOow08SrMoo7"
      },
      "source": [
        "# df_load = pd.read_json(\"/IMDB_reviews.json\", lines=True)\n",
        "\n",
        "# df_shuffled = df_load.sample(frac=1)\n",
        "\n",
        "# df_true = df_shuffled[df_shuffled['is_spoiler'] == True]\n",
        "# df_false = df_shuffled[df_shuffled['is_spoiler'] == False]\n",
        "\n",
        "# df_sliced = df_true.append(df_false)\n",
        "# tfds_spoiler = (tf.data.Dataset.from_tensor_slices(\n",
        "#     (\n",
        "#     df_sliced['review_text'].values,\n",
        "#     df_sliced['is_spoiler'].values\n",
        "#     )\n",
        "# )\n",
        "# )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SripIh6nb9-i"
      },
      "source": [
        "# len(tfds_spoiler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_TQuz7qA83_"
      },
      "source": [
        "Mega dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4CysDVvAbxf"
      },
      "source": [
        "path = \"/content/drive/MyDrive/(H)ydrogen (u)ranium (ge)rmanium (b)oron (ra)idum (in)dium (s)ulfur/Colab Notebooks/Mega_IMDB.zip\"\n",
        "if not os.path.isfile(\"/sample.json\"):\n",
        "  with zipfile.ZipFile(path, 'r') as zip_ref:\n",
        "      zip_ref.extractall(\"/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zCsYIzpAVGf"
      },
      "source": [
        "df_load1 = pd.read_json(\"/part-01.json\")\n",
        "df_load2 = pd.read_json(\"/part-02.json\")\n",
        "df_load3 = pd.read_json(\"/part-03.json\")\n",
        "df_load4 = pd.read_json(\"/part-04.json\")\n",
        "df_load5 = pd.read_json(\"/part-05.json\")\n",
        "df_load6 = pd.read_json(\"/part-06.json\")\n",
        "\n",
        "frames = [df_load1, df_load2, df_load3, df_load4, df_load5, df_load6]\n",
        "\n",
        "df_load_big = pd.concat(frames, ignore_index=True)\n",
        "\n",
        "del df_load1, df_load2, df_load3, df_load4, df_load5, df_load6\n",
        "\n",
        "df_shuffled = df_load_big.sample(frac=1)\n",
        "\n",
        "del df_load_big\n",
        "\n",
        "df_true = df_shuffled[df_shuffled['spoiler_tag'] == 1].head(1100000)\n",
        "df_false = df_shuffled[df_shuffled['spoiler_tag'] == 0].head(1100000)\n",
        "\n",
        "del df_shuffled\n",
        "\n",
        "df_sliced = df_true.append(df_false)\n",
        "\n",
        "del df_true, df_false\n",
        "\n",
        "tfds_spoiler = (tf.data.Dataset.from_tensor_slices(\n",
        "    (\n",
        "    df_sliced['review_detail'].values,\n",
        "    df_sliced['spoiler_tag'].values\n",
        "    )\n",
        ")\n",
        ")\n",
        "\n",
        "del df_sliced"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqTFwsYdZg_e"
      },
      "source": [
        "len(tfds_spoiler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rXHa-w9JZhb"
      },
      "source": [
        "Import `matplotlib` and create a helper function to plot graphs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mp1Z7P9pYRSK"
      },
      "source": [
        "def plot_graphs(history, metric):\n",
        "  plt.plot(history.history[metric])\n",
        "  plt.plot(history.history['val_'+metric], '')\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(metric)\n",
        "  plt.legend([metric, 'val_'+metric])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRmMubr0jrE2"
      },
      "source": [
        "Setup input pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHRwRoP2nVHX"
      },
      "source": [
        "train_size = int(0.75 * len(tfds_spoiler))\n",
        "test_size = int(0.25 * len(tfds_spoiler))\n",
        "\n",
        "tfds_spoiler = tfds_spoiler.shuffle(len(tfds_spoiler))\n",
        "train_spoiler = tfds_spoiler.take(train_size)\n",
        "test_spoiler = tfds_spoiler.skip(train_size)\n",
        "test_spoiler = test_spoiler.take(test_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWA4c2ir7g6p"
      },
      "source": [
        "Initially this returns a dataset of (text, label pairs):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vd4_BGKyurao"
      },
      "source": [
        "spoiler_true = 0\n",
        "spoiler_false = 0\n",
        "for example, label in test_spoiler:\n",
        "  if label.numpy() == True:\n",
        "    spoiler_true += 1\n",
        "  else:\n",
        "    spoiler_false += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArN49NAerL80"
      },
      "source": [
        "print(spoiler_true, spoiler_false)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2qVJzcEluH_"
      },
      "source": [
        "Next shuffle the data for training and create batches of these `(text, label)` pairs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDsCaZCDYZgm"
      },
      "source": [
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VznrltNOnUc5"
      },
      "source": [
        "train_dataset = train_spoiler.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = test_spoiler.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqkvdcFv41wC"
      },
      "source": [
        "for example, label in train_dataset.take(1):\n",
        "  print('texts: ', example.numpy()[:3])\n",
        "  print()\n",
        "  print('labels: ', label.numpy()[:3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5eWCo88voPY"
      },
      "source": [
        "## Create the text encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uC25Lu1Yvuqy"
      },
      "source": [
        "VOCAB_SIZE=2000\n",
        "encoder = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE)\n",
        "encoder.adapt(train_dataset.map(lambda text, label: text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FaEcTo0UWOs"
      },
      "source": [
        "def vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return encoder(text), label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ALSA-AyUpL1"
      },
      "source": [
        "vec_train_dataset = train_dataset.map(vectorize_text)\n",
        "vec_test_dataset = test_dataset.map(vectorize_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omKUX7NEVDtf"
      },
      "source": [
        "for example, label in vec_train_dataset.take(1):\n",
        "  print('texts: ', example.numpy()[:3])\n",
        "  print()\n",
        "  print('labels: ', label.numpy()[:3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBoyjjWg0Ac9"
      },
      "source": [
        "vocab = np.array(encoder.get_vocabulary())\n",
        "vocab[:20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjUqGVBxGw-t"
      },
      "source": [
        "## Create and train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPjgBNNK_4Xe"
      },
      "source": [
        "# model = tf.keras.models.load_model(\"/content/drive/MyDrive/ML_Model\")\n",
        "\n",
        "# model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jo1jjO3vn0jo"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 64, mask_zero=True),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEPV5jVGp-is"
      },
      "source": [
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeSE-YjdqAeN"
      },
      "source": [
        "history = model.fit(vec_train_dataset, epochs=15,\n",
        "                    validation_data=vec_test_dataset,\n",
        "                    validation_steps=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YYub0EDtwCu"
      },
      "source": [
        "plt.figure(figsize=(16,6))\n",
        "plt.subplot(1,2,1)\n",
        "plot_graphs(history, 'accuracy')\n",
        "plt.subplot(1,2,2)\n",
        "plot_graphs(history, 'loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LdwilM1qPM3"
      },
      "source": [
        "test_loss, test_acc = model.evaluate(vec_test_dataset)\n",
        "\n",
        "print('Test Loss: {}'.format(test_loss))\n",
        "print('Test Accuracy: {}'.format(test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFkFlc_R9_yc"
      },
      "source": [
        "Predict on a sample text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykUKnAoqbycW"
      },
      "source": [
        "sample_text = ('The Avengers go back in time to get the Infinity Stones before they are found at various times in the MCU (Natasha sacrifices herself so Clint can get the Soul Stone). Stark develops a gauntlet which the Hulk puts on and uses to snap back the beings who were killed by the original snap. Thanos arrives and wages a full on war against all the heroes from the MCU movies. When Stark and Thanos fight, Stark takes the stones and uses them to eliminate Thanos and his army so that the universe may live in peace. The power of the stones is too much, and Stark dies. After the funeral, Banner and Wilson help Rogers go back in time to return the stones to their places of origin. Rogers returns as an old man to give the shield to Wilson. We are then shown Rogers dancing with Peggy Carter, and they kiss as the movie ends. There are no mid or end credit scenes.')\n",
        "predictions = model.predict([encoder([sample_text])])\n",
        "print(predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsJ_1Ys8AuZa"
      },
      "source": [
        "Save and export model to tensorflow JS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTZSxpRogZFU"
      },
      "source": [
        "!pip install tensorflowjs\n",
        "import tensorflowjs as tfjs\n",
        "model.save(\"/content/drive/MyDrive/ML_Model\")\n",
        "tfjs.converters.save_keras_model(model, \"/content/drive/MyDrive/JSON_Model/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYC-Gf1PAzrX"
      },
      "source": [
        "Save the vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kusNjvmronQX"
      },
      "source": [
        "with open(\"/content/drive/MyDrive/JSON_Model/vocabulary.txt\", \"w\") as write_file:\n",
        "  for line in vocab:\n",
        "    write_file.write(line + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}